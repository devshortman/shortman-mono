# backend/backend/simple_crawler.py
"""
ì§€ì—­ë³„ Shorts í¬ë¡¤ëŸ¬
- í•œêµ­: Instagram 4 + YouTube 4 + TikTok 4 = 12ê°œ
- í•´ì™¸: Instagram 4 + YouTube 4 + TikTok 4 = 12ê°œ  
- ì¤‘êµ­: Instagram 4 + YouTube 4 + TikTok 4 = 12ê°œ
ì´ 36ê°œ ìˆ˜ì§‘
"""

import os
import logging
from datetime import datetime, timezone
from typing import List, Dict, Any
from supabase import create_client, Client
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

# Supabase í´ë¼ì´ì–¸íŠ¸
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    raise RuntimeError("Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


class RegionalCrawler:
    """ì§€ì—­ë³„ í¬ë¡¤ëŸ¬"""
    
    @staticmethod
    def fetch_region_shorts(region: str, count: int = 12) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ì§€ì—­ì˜ Shorts ìˆ˜ì§‘
        region: 'korea', 'global', 'china'
        """
        logging.info(f"ğŸŒ Fetching {count} shorts from {region.upper()}...")
        
        items = []
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        
        # ê° í”Œë«í¼ì—ì„œ 4ê°œì”©
        platforms = [
            ('instagram', 'ğŸ“¸', 'https://www.instagram.com/reel/'),
            ('youtube', 'ğŸ¥', 'https://youtube.com/shorts/'),
            ('tiktok', 'ğŸµ', 'https://www.tiktok.com/@user/video/')
        ]
        
        for platform, emoji, url_base in platforms:
            for i in range(4):
                item_id = f"{platform}_{region}_{timestamp}_{i}"
                
                items.append({
                    'platform': platform,
                    'platform_id': item_id,
                    'region': region,
                    'title': f"{emoji} {region.upper()} {platform.capitalize()} Trend #{i+1}",
                    'nickname': f'{platform}_user_{region}_{i+1}',
                    'avatar': f'https://picsum.photos/seed/{item_id}/100/100',
                    'thumbnail': f'https://picsum.photos/seed/{item_id}/400/600',
                    'video_url': f'{url_base}{item_id}',
                    'description': f'{region.capitalize()} trending video on {platform.capitalize()}',
                    'likes': 10000 + (i * 1000),
                    'views': 50000 + (i * 5000),
                    'comments': 500 + (i * 50),
                    'published_at': datetime.now(timezone.utc).isoformat(),
                    'crawled_at': datetime.now(timezone.utc).isoformat(),
                })
        
        logging.info(f"âœ… Collected {len(items)} items from {region.upper()}")
        return items


class SupabaseWriter:
    """Supabaseì— ë°ì´í„° ì €ì¥"""
    
    @staticmethod
    def save_items(items: List[Dict[str, Any]]) -> int:
        """
        ì•„ì´í…œ ì €ì¥ (ì¤‘ë³µ ì œê±°)
        platform + platform_id ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
        """
        if not items:
            logging.warning("No items to save")
            return 0
        
        saved_count = 0
        for item in items:
            try:
                # ì¤‘ë³µ ì²´í¬
                existing = supabase.table('shorts_items').select('id').eq(
                    'platform', item['platform']
                ).eq(
                    'platform_id', item['platform_id']
                ).execute()
                
                if existing.data:
                    logging.info(f"â­ï¸  Skip duplicate: {item['platform']} - {item['region']}")
                    continue
                
                # ìƒˆ í•­ëª© ì €ì¥
                result = supabase.table('shorts_items').insert(item).execute()
                saved_count += 1
                logging.info(f"ğŸ’¾ Saved: {item['region']} - {item['platform']} - {item['title']}")
                
            except Exception as e:
                logging.error(f"âŒ Failed to save {item['platform_id']}: {e}")
        
        return saved_count


def run_crawler():
    """í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰"""
    logging.info("=" * 80)
    logging.info("ğŸš€ Starting Regional Shorts Crawler")
    logging.info("=" * 80)
    
    crawler = RegionalCrawler()
    
    # 1. í•œêµ­ (12ê°œ)
    korea_items = crawler.fetch_region_shorts('korea', 12)
    
    # 2. í•´ì™¸/ê¸€ë¡œë²Œ (12ê°œ)
    global_items = crawler.fetch_region_shorts('global', 12)
    
    # 3. ì¤‘êµ­ (12ê°œ)
    china_items = crawler.fetch_region_shorts('china', 12)
    
    all_items = korea_items + global_items + china_items
    
    logging.info("\n" + "=" * 80)
    logging.info(f"ğŸ“Š Collection Summary:")
    logging.info(f"   ğŸ‡°ğŸ‡· Korea:  {len(korea_items)} items")
    logging.info(f"   ğŸŒ Global: {len(global_items)} items")
    logging.info(f"   ğŸ‡¨ğŸ‡³ China:  {len(china_items)} items")
    logging.info(f"   ğŸ“¦ Total:  {len(all_items)} items")
    logging.info("=" * 80)
    
    # Supabaseì— ì €ì¥
    logging.info("\nğŸ’¾ Saving to Supabase...")
    writer = SupabaseWriter()
    saved_count = writer.save_items(all_items)
    
    logging.info("\n" + "=" * 80)
    logging.info(f"âœ… Crawler Finished!")
    logging.info(f"   ğŸ“¥ Total collected: {len(all_items)}")
    logging.info(f"   âœ¨ Newly saved:    {saved_count}")
    logging.info(f"   â­ï¸  Duplicates:     {len(all_items) - saved_count}")
    logging.info("=" * 80)
    
    return saved_count


if __name__ == "__main__":
    run_crawler()
